\documentclass[12pt,a4paper]{article}

\usepackage{amsmath}
\usepackage{palatino}
\usepackage{lipsum}
\usepackage{mwe}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{color}
\usepackage{amssymb}
\usepackage{float}
\usepackage{amsfonts}
\usepackage{subfigure}
\usepackage{apacite}
\usepackage{multirow}

\begin{document}
	
\title{STA3007 Assignment 2}
\author{118010350}
\date{\today}
\maketitle	
	
\newpage

\textbf{Question 1}\\
~\\
(a) T: The arrangement of $\{r_{ij}\}$ is a permutation of $\{1,2,\cdots,N\}$, hence totally $N!$ ways to arrange ranks. The order of ranks within each treatment $j$ has no effect on $H$, we can fix the order as $r_{1j}<r_{2j}<\cdots<r_{n_{j}j},j=1,\cdots,k$, to determine the distribution of $H$. But $n_{1}=\cdots=n_{k}=n$, we have $$H=\frac{12}{N(N+1)}\sum_{j=1}^{k}\frac{R_{j}^{2}}{n}-3(N+1)$$ Then $H$ is invariant with the order of $\{R_{1},\cdots,R_{k}\}$. Hence it requires $${N\choose n_{1},\cdots,n_{k}}/k!=\frac{(nk)!}{k!(n!)^{k}}$$ rank assignments to determine the exact distribution of the test statistic $H$.\\
~\\
(b) F: If $k=3$, $n_{1}=n_{2}=n_{3}=2$, then the arrangement of $\{r_{ij}\}$ is a permutation of $\{1,2,3,4,5,6\}$, hence totally $6!$ ways to arrange ranks. The order of ranks within each treatment $j$ has no effect on $J$, we can fix the order as $r_{1j}<r_{2j},j=1,2,3$, to determine the distribution of $J$. The statistic $J$ is given by $$J=\sum_{u<v}U_{uv}=U_{12}+U_{13}+U_{23}$$ where $U_{uv}$ represents the number of pairs $(X_{iu},X_{jv})$ such that $X_{iu}<X_{jv}$, which is equivalent to $r_{iu}<r_{jv}$. But the order of $R_{j}$ has an effect on $U_{uv}$, that is, $J$ is variant with the order of $\{R_{1},R_{2},R_{3}\}$. Hence the test statistic $J$ can be determined by $$\frac{6!}{(2!)^{3}}=90$$ rank assignments.\\
~\\
(C) F: The alternative hypothesis of the Jonckheere-Terpstra test is $H_{1}:\tau_{1}\leqslant\cdots\leqslant\tau_{k}$ and $\tau_{i}<\tau_{j}$ for at least one pair $i<j$, while the alternative hypothesis of the Kruskal-Wallis test is $H_{1}:\tau_{1},\cdots,\tau_{k}$ are not all equal. 
The Jonckheere-Terpstra test is superior to the Kruskal-Wallis test when the conjectured ordering of the treatment effects is indeed appropriate. 

Hence under the appropriate ordering $\tau_{1}\leqslant\cdots\leqslant\tau_{k}$, the Jonckheere-Terpstra test is powerful to reject the null hypothesis, but the Kruskal-Wallis test is not as powerful as the Jonckheere-Terpstra test, which may come to the contradictory result that accept the null hypothesis.

Though the Jonckheere-Terpstra test rejects the null hypothesis $H_{0}:\tau_{1}=\cdots=\tau_{k}$ at the level $\alpha$ of significance, the Kruskal-Wallis test may accept the null hypothesis.

~\\
\indent \textbf{Question 2}\\
~\\
(a) F: The hypothesis test defines a null hypothesis $H_{0}$ and an alternative hypothesis $H_{1}$. The null hypothesis of the multiple comparisons is $H_{0}:\tau_{1}=\cdots=\tau_{k}$, but the alternative hypothesis $H_{1}$ is specified by the decision rule: decide $\tau_{u}\neq \tau_{v}$ if $|W_{uv}^{\ast}|\geqslant w_{\alpha}^{\ast}$ otherwise accept $\tau_{u}=\tau_{v}$. The alternative hypothesis is not defined before the multiple comparison procedure. Hence the multiple comparisons cannot be considered as a hypothesis test.\\
~\\
(b) T: Since $W_{ij}$ is asymptotically normal, then $W_{ij}$ can be standardized for large samples, that is, $\frac{W_{ij}-E_{0}[W_{ij}]}{\sqrt{Var_{0}(W_{ij})}}\sim N(0,1)$ for large samples. By definition, $W_{ij}^{\ast}$ is given by $$W_{ij}^{\ast}=\frac{W_{ij}-E_{0}[W_{ij}]}{\sqrt{Var_{0}(W_{ij})/2}}$$Then we have $W_{ij}^{\ast}\sim N(0,2)$ for large samples. 

Since $Z_{i}$, $Z_{j}$ are independent variables, where $Z_{i}\sim N(0,\frac{1}{n_{i}})$, $Z_{j}\sim N(0,\frac{1}{n_{j}})$, then $$E\left(\frac{Z_{i}-Z_{j}}{\sqrt{(n_{i}+n_{j})/(2n_{i}n_{j})}}\right)=\frac{E(Z_{i})-E(Z_{j})}{\sqrt{(n_{i}+n_{j})/(2n_{i}n_{j})}}=0$$ and $Cov(Z_{i},Z_{j})=0$, then $$Var\left(\frac{Z_{i}-Z_{j}}{\sqrt{(n_{i}+n_{j})/(2n_{i}n_{j})}}\right)=\frac{Var(Z_{i})-Var(Z_{j})}{(n_{i}+n_{j})/(2n_{i}n_{j})}=2$$ For $Z_{i}$, $Z_{j}$ are both normal and independent, then $\frac{Z_{i}-Z_{j}}{\sqrt{(n_{i}+n_{j})/(2n_{i}n_{j})}}$ is also normal. Then $\frac{Z_{i}-Z_{j}}{\sqrt{(n_{i}+n_{j})/(2n_{i}n_{j})}}\sim N(0,2)$. Hence, $W_{ij}^{\ast}$ and $\frac{Z_{i}-Z_{j}}{\sqrt{(n_{i}+n_{j})/(2n_{i}n_{j})}}$ follow the same distribution, that is, $W_{ij}^{\ast} \sim \frac{Z_{i}-Z_{j}}{\sqrt{(n_{i}+n_{j})/(2n_{i}n_{j})}}$ approximately for large samples.\\
~\\
(c) F: The SDCF two-sided all-treatment multiple comparison procedure decides $\tau_{1}=\tau_{2}$, $\tau_{1}=\tau_{3}$, $\tau_{2}\neq\tau_{3}$ at $\alpha=0.05$ exact. In the multiple comparison procedure, by definition,  $$Pr(|W_{uv}^{*}|<w_{0.05}^{*},1\leqslant u\leqslant v\leqslant 3)=0.95$$ under $H_{0}:\tau_{1}=\tau_{2}=\tau_{3}$. Then we have $$Pr\left(|W_{uv}^{*}|\geqslant w_{0.05}^{*}\text{ for some }1\leqslant u\leqslant v\leqslant 3\right)=0.05$$
under $H_{0}:\tau_{1}=\tau_{2}=\tau_{3}$. 

The equations state that the probability of making all correct decisions when $H_{0}$ is true is controlled to be $1-\alpha=0.95$, that is, the probability of at least one incorrect decision, when $H_{0}$ is true, is controlled to be $\alpha=0.05$. 

Hence we can claim that the probability of error to decide $\tau_{1}=\tau_{2}$, $\tau_{1}=\tau_{3}$, $\tau_{2}\neq\tau_{3}$ is $\alpha=0.05$ under $H_{0}:\tau_{1}=\tau_{2}=\tau_{3}$.\\
~\\
~\\
\indent \textbf{Question 3}\\
~\\
(a) F: The inequality is equivalent to $$\sum_{j=1}^{g_{i}}t_{i,j}<k$$ where $g_{i}$ is the number of tied groups in block $i$, $t_{i,j}$ is the size of the $j$-th tied group in block $i$, $j=1,\cdots,g_{i}$, $i=1,\cdots,n$. We note that an untied observation within a block is considered to be a tied group of size $1$. Then in the case of complete block design, $$\sum_{j=1}^{g_{i}}t_{i,j}=k$$. Hence we correct the original inequality as $$\sum_{j=1}^{g_{i}}t_{i,j}^{3}-k=\sum_{j=1}^{g_{i}}t_{i,j}(t_{i,j}-1)(t_{i,j}+1)$$\\
~\\
(b) T: The Page test statistic for ordered alternative is given by $$L=\sum_{j=1}^{k}jR_{j}=R_{1}+2R_{2}+\cdots+kR_{k}$$ Consider the smallest value of $L$, that is, $r_{ij}=j$ for any $i=1,\cdots,n$. Then the smallest $L$ is  $$L=\sum_{j=1}^{k}j(nj)=n(1^{2}+2^{2}+\cdots+k^{2})=\frac{nk(k+1)(2k+1)}{6}$$ Since $E_{0}(L)\frac{nk(k+1)^{2}}{4}$, then $2E_{0}(L)/3=\frac{nk(k+1)^{2}}{6}$. Then we can know that the smallest $L$ is always greater than $2E_{0}(L)/3$. Hence the Page test statistic is always greater than $2E_{0}(L)/3$.\\
~\\
(c) T: Since each block has an equal number of observations, then $s_{i}=s$, $i=1,\cdots,n$. By the definition of $A_{j}$, we have $$A_{j}=\sum_{i=1}^{n}\sqrt{\frac{12}{s+1}}(r_{ij}-\frac{s+1}{2})=\sqrt{\frac{12}{s+1}}R_{j}-n\sqrt{3(s+1)}$$ for $j=1,\cdots,k$. Let $a=\sqrt{\frac{12}{s+1}}$, $b=-n\sqrt{3(s+1)}$, then $A_{j}=aR_{j}+b$, $j=1,\cdots,k$.

~\\
\indent \textbf{Question 4}\\
~\\
(a) Test the null hypothesis $H_{0}: Var(X)=Var(Y)$ against $H_{1}:Var(X)\neq Var(Y)$ by the Miller's Jackknife test. 

We have $X=(X_{1},\cdots,X_{11})$ and $Y=(Y_{1},\cdots,Y_{10})$, where $m=11$, $n=10$. Then
\begin{align*}
\bar{X}_{0}&=\frac{1}{m}\sum_{i=1}^{m}X_{i}=8.05 \text{, } D_{0}^{2}=\frac{1}{m-1}\sum_{i=1}^{m}(X_{i}-\bar{X}_{0})^{2}=22.24\\
\bar{Y}_{0}&=\frac{1}{n}\sum_{j=1}^{n}Y_{j}=10.91 \text{, } E_{0}^{2}=\frac{1}{n-1}\sum_{i=1}^{n}(Y_{i}-\bar{Y}_{0})^{2}=22.71
\end{align*} 
Since $\bar{X}_{i}=\frac{1}{m-1}\sum_{s\neq i}^{m}X_{s}$, $\bar{Y}_{j}=\frac{1}{n-1}\sum_{t\neq j}^{n}Y_{t}$, then
\begin{align*}
(\bar{X}_{1},\cdots,\bar{X}_{11})&=(8.11,8.51,7.18,8.48,8.4,8.58,8.25,7.29,7.98,7.94,7.88)\\
(\bar{Y}_{1},\cdots,\bar{Y}_{10})&=(10.5,11.84,10.83,11.88,10.76,10.71,11,10.56,10.4,10.62)
\end{align*}
Since $D_{i}^{2}=\frac{1}{m-2}\sum_{s\neq i}^{m}(X_{s}-\bar{X}_{i})^{2}$, $E_{j}^{2}=\frac{1}{n-2}\sum_{t\neq j}^{m}(Y_{t}-\bar{Y}_{j})^{2}$, then
\begin{align*}
(D_{1}^{2},\cdots,D_{11}^{2})&=(24.67,22.17,15.36,22.50,23.25,21.33,24.24,17.56,24.64,24.55,24.34)\\
(E_{1}^{2},\cdots,E_{10}^{2})&=(23.66,15.73,25.49,15.02,25.29,25.11,25.46,24.15,22.63,24.62)
\end{align*}
Since $S_{i}=logD_{i}^{2}$, $T_{j}=logE_{j}^{2}$, then
\begin{align*}
(S_{0},\cdots,S_{11})&=(3.10,3.21,3.10,2.73,3.11,3.15,3.06,3.19,2.87,3.20,3.20,3.19)\\
(T_{0},\cdots,T_{10})&=(3.12,3.16,2.76,3.24,2.71,3.23,3.22,3.24,3.18,3.12,3.20)
\end{align*}
Since $A_{i}=mS_{0}-(m-1)S_{i}$, $B_{j}=nT_{0}-(n-1)T_{j}$, then
\begin{align*}
(A_{1},\cdots,A_{11})&=(2.06,3.13,6.80,2.99,2.66,3.52,2.24,5.46,2.08,2.11,2.20)\\
(B_{1},\cdots,B_{10})&=(2.75,6.43,2.09,6.85,2.16,2.22,2.10,2.57,3.16,2.40)
\end{align*}
The value of $\bar{A}$ and $\bar{B}$ are given by $$\bar{A}=\frac{1}{m}\sum_{i=1}^{m}A_{i}=3.20 \text{, } \bar{B}=\frac{1}{n}\sum_{j=1}^{n}B_{j}=3.27$$
The Jackknife estimates of the variances of $\bar{A}$ and $\bar{B}$ are given by $$V_{1}=\sum_{i=1}^{m}\frac{(A_{i}-\bar{A})^{2}}{m(m-1)}=0.22 \text{, } V_{2}=\sum_{j=1}^{n}\frac{(B_{i}-\bar{B})^{2}}{n(n-1)}=0.33$$
The absolute value of the test statistic $Q$, which is given by
$$|Q|=\left|\frac{\bar{A}-\bar{B}}{\sqrt{V_{1}+V_{2}}}\right|=\left|\frac{3.20-3.27}{\sqrt{0.22+0.33}}\right|=0.092$$
The test statistic $Q\sim N(0,1)$ approximately, and $z_{\alpha/2}=z_{0.4}=0.253>|Q|$. 

Hence we accept the null hypothesis $H_{0}:Var(X)=Var(Y)$ at 80\% level of significance.

~\\
(b) Test the null hypothesis $H_{0}:\theta_{1}=\theta_{2}$ and $\eta_{1}=\eta_{2}$ against $H_{1}:$ either $\theta_{1}\neq \theta_{2}$ or $\eta_{1} \neq \eta_{2}$ (or both) by the Lepage test.

The ordered values $Z_{(1)}<Z_{(2)}<\cdots<Z_{(21)}$ of combined $(X,Y)$ are 
\begin{align*}
&2.2, 2.5, 2.8, 3.5, 3.8, 4.6, 6.1, 7.5, 8.8, 9.2, 9.8, 10.1,\\
&11.6, 12.3, 12.7, 13.5, 14.1, 14.6, 15.5, 15.7, 16.8
\end{align*}

The ranks of $Z_{(1)},Z_{(2)},\cdots,Z_{(21)}$ are $(1, 2,\cdots,21)$, and the scores of $Z_{(1)},$\\$Z_{(2)},\cdots,Z_{(21)}$ are $(1,\cdots,10,11,10,\cdots,1)$. Then the Wilcoxon rank sum $W$ and the Ansari-Bradley statistic $C$ are given by
\begin{align*}
W&=1+2+12+13+14+15+16+17+18+19=127\\
C&=1+2+10+9+8+7+6+5+4+3=55
\end{align*}
Since $m=11$, $n=10$ and $N=21$, the means and variances of $W$ and $C$ under the null hypothesis are 
\begin{align*}
&E_{0}(W)=\frac{n(N+1)}{2}=\frac{10(21+1)}{2}=110\\
&Var_{0}(W)=\frac{mn(N+1)}{12}=\frac{11(10)(21+1)}{12}=\frac{605}{3}\\
&E_{0}(C)=\frac{n(N+1)^{2}}{4N}=\frac{10(21+1)^{2}}{4(21)}=\frac{1210}{21}\\
&Var_{0}(C)=\frac{mn(N+1)(N^{2}+3)}{48N^{2}}=\frac{11(10)(21+1)(21^{2}+3)}{48(21^{2})}=\frac{22385}{441}
\end{align*}
Thus the Lepage test statistic for location or dispersion is
\begin{align*}
D=(W^{*})^{2}+(C^{*})^{2}&=\frac{(W-E_{0}(W))^{2}}{Var_{0}(W)}+\frac{(C-E_{0}(C))^{2}}{Var_{0}(C)}\\
&=1.433+0.135\\
&=1.568
\end{align*}
The test statistic $D\sim \chi_{2}^{2}$ approximately, and $\chi_{2,\alpha}^{2}=\chi_{2,0.4}^{2}=1.833>D$.

Hence we accept the null hypothesis $H_{0}:\theta_{1}=\theta_{2}$ and $\eta_{1}=\eta_{2}$, that is, there is no difference in location and/or dispersion parameters between two samples, at 40\% level of significance.

~\\
(c) The empirical distribution functions of $F(t)$ and $G(t)$ is given by $F_{m}(t)=\dfrac{1}{m}\sum_{i=1}^{m}I_{\{X_{i}\leqslant t\}}$ and $G_{n}(t)=\dfrac{1}{n}\sum_{j=1}^{n}I_{\{Y_{j}\leqslant t\}}$. 

Then we can calculate the values of $F_{m}(t)$ and $G_{n}(t)$ at ordered values $Z_{(1)}<Z_{(2)}<\cdots<Z_{(21)}$ of combined $(X,Y)$.

The values of $F_{11}(t)$ for $t=Z_{(1)},\cdots,Z_{(21)}$ are
\begin{align*}
(F_{11}(Z_{(1)}),\cdots,F_{11}(Z_{(21)})=&(0,0,1/11,2/11,3/11,4/11,5/11,6/11,7/11,8/11,\\
9/1&1,9/11,9/11,9/11,9/11,9/11,9/11,9/11,9/11,10/11,1)
\end{align*}

The values of $G_{10}(t)$ for $t=Z_{(1)},\cdots,Z_{(21)}$ are
\begin{align*}
(G_{10}(Z_{(1)}),\cdots,G_{10}(Z_{(21)}))=&(1/10,2/10,2/10,2/10,2/10,2/10,2/10,2/10,2/10,\\
2/&10,2/10,3/10,4/10,5/10,6/10,7/10,8/10,9/10,1,1,1)
\end{align*}

Then by the definition of Two-sample Kolmogorov-Smirnov test statistic, and $m=11$, $n=10$, $d=1$, we can get
\begin{align*}
J&=\frac{11\times 10}{1}\max_{1\leqslant i\leqslant 21}|F_{11}(Z_{(i)})-G_{10}(Z_{(i)})|\\&=110|F_{11}(Z_{(11)})-G_{10}(Z_{(11)})|\\&=110\left|\frac{9}{11}-\frac{2}{10}\right|=68
\end{align*}
Then we use the R program to obtain the $p$-value of the test.\\

$>$ x$<$-c(7.5, 3.5, 16.8, 3.8, 4.6, 2.8, 6.1, 15.7, 8.8, 9.2, 9.8)\\
\indent $>$ y$<$-c(14.6, 2.5, 11.6, 2.2, 12.3, 12.7, 10.1, 14.1, 15.5, 13.5)\\
\indent $>$ ks.test(x,y)\\
\indent $>$ ansari.test(y, x)\\
~\\
\indent\indent\indent\indent\indent\indent Two-sample Kolmogorov-Smirnov test\\
\indent data: x and y\\
\indent D = 0.61818, p-value = 0.02421\\
\indent alternative hypothesis: two-sided\\
~\\
The $p$-value $=0.024<0.05$, which shows that there is sufficient evidence for general differences between the distribution of $X$ and $Y$ at 5\% level of significance.

~\\
(d) Based on the results of parts (a)-(c), we can know that, under the location-scale parameter model, there is no significant difference in location and/or dispersion between $X$ and $Y$. However, without the assumption of location-scale parameter model, there are general differences between $X$ and $Y$. Hence,

1) The overall differences between the two samples are the distribution forms which are not suitable for the location-scale parameter model simultaneously;

2) The location-scale parameter model is not appropriate here, since the test in (c) detects the differences between two samples, while the tests in (a) and (b) do not. The results show that these two samples do not follow the location-scale parameter model simultaneously.

~\\
\indent \textbf{Question 5}\\
~\\
(a) Test $H_{0}$ against $H_{1}:\tau_{1}\leqslant\tau_{2}\leqslant\tau_{3}\leqslant\tau_{4}\leqslant\tau_{5}$ with at least one strict inequality by the Jonckheere-Terpstra test.

Based on the values of $U_{uv}$, $1\leqslant u<v\leqslant 5$, the Jonckheere-Terpstra test statistic is given by 
\begin{align*}
J=\sum_{u<v}U_{uv}&=U_{12}+U_{13}+U_{14}+U_{15}+U_{23}\\
&+U_{24}+U_{25}+U_{34}+U_{35}+U_{45}=213
\end{align*}
Use the exact rejection rule by R program, where $(n_{1},\cdots,n_{5})=(6,5,7,4,6)$.\\

\indent $>$ cJCK(0.01,c(6,5,7,4,6))\\
~\\
\indent Group sizes: 6 5 7 4 6\\
\indent For the given alpha=0.05, the upper cutoff value is Jonckheere-Terpstra\\
\indent J=213, with true alpha level=0.0099\\
~\\
Since $Pr(J\geqslant213)=0.0099<0.01$, then we reject the null hypothesis $H_{0}:\tau_{1}=\cdots=\tau_{5}$ at 1\% level of significance.

Based on $N=n_{1}+\cdots+n_{5}=28$, the mean and variance of $J$ under $H_{0}$ are given by
\begin{align*}
E_{0}(J)=&\frac{1}{4}\left(N^{2}-\sum_{u=1}^{5}n_{u}^{2}\right)\\
=&\frac{1}{4}\left(28^{2}-6^{2}-5^{2}-7^{2}-4^{2}-6^{2}\right)=\frac{311}{2}\\
Var_{0}(J)=&\frac{1}{72}\left[N^{2}(2N+3)-\sum_{u=1}^{5}n_{u}^{2}(2n_{u}+3)\right]\\
=&\frac{1}{72}[28^{2}(56+3)-6^{2}(12+3)-5^{2}(10+3)\\
&-7^{2}(14+3)-4^{2}(8+3)-6^{2}(12+3)]=\frac{7303}{12}
\end{align*}
Use the approximate rejection rule. The value of $J^{*}$ is given by
$$J^{*}=\frac{J-E_{0}(J)}{\sqrt{Var_{0}(J)}}=\frac{213-311/2}{\sqrt{7303/12}}=2.330$$
When the sample size is large, $J^{*}\sim N(0,1)$ approximately. Since $z_{\alpha}=z_{0.01}=2.326<J^{*}$, hence we reject the null hypothesis $H_{0}:\tau_{1}=\cdots=\tau_{5}$ at 1\% level of significance.

~\\
(b) Test $H_{0}$ against $H_{1}:\tau_{1}\leqslant\tau_{2}\leqslant\tau_{3}\geqslant\tau_{4}\geqslant\tau_{5}$ with at least one strict inequality by the Mack-Wolfe test, where the known peak $p=3$.

Since $U_{vu}=n_{u}n_{v}-U_{uv}$ for $u<v$, then we have
\begin{align*}
&U_{43}=7\times 4-16=12\\
&U_{53}=7\times 6-16=26\\
&U_{54}=4\times 6-10=14
\end{align*}
The Mack-Wolfe test statistic is given by
\begin{align*}
A_{3}&=\sum_{u<v\leqslant 3}U_{uv}+\sum_{3\leqslant u<v}U_{vu}\\
&=U_{12}+U_{13}+U_{23}+U_{43}+U_{53}+U_{54}\\
&=26+36+28+12+26+14\\
&=142
\end{align*}
Use the exact rejection rule by R program.\\

\indent $>$ cUmbrPK(0.05,c(6,5,7,4,6),3)\\
~\\
\indent Group sizes: 6 5 7 4 6\\
\indent For the given alpha=0.05, the upper cutoff value is Mack-Wolfe Peak\\
\indent Known A 3=135, with true alpha level=0.0462\\
~\\
Since $Pr(A_{3}\geqslant 135)=0.046$, then $Pr(A_{3}\geqslant 142)<0.05$. Hence we reject the null hypothesis $H_{0}:\tau_{1}=\cdots=\tau_{5}$ at 5\% level of significance.

Based on $N_{1}=n_{1}+n_{2}+n_{3}=18$, $N_{2}=n_{3}+n_{4}+n_{5}=17$, the mean and variance of $A_{3}$ under $H_{0}$ are given by
\begin{align*}
E_{0}(A_{3})=&\frac{1}{4}\left(N_{1}^{2}+N_{2}^{2}-\sum_{i=1}^{5}n_{i}^{2}-n_{3}^{2}\right)\\
=&\frac{1}{4}(18^{2}+17^{2}-2\times 6^{2}-5^{2}-2\times 7^{2}-4^{2})=\frac{201}{2}\\
Var_{0}(A_{3})=&\frac{1}{72}\left[2(N_{1}^{3}+N_{2}^{3})+3(N_{1}^{2}+N_{2}^{2})-\sum_{i=1}^{5}n_{i}^{2}(2n_{i}+3)-n_{3}^{2}(2n_{3}+3)\right]\\
&+\frac{1}{6}(n_{3}N_{1}N_{2}-n_{3}^{2}N)\\
=&\frac{1}{72}[2(18^{3}+17^{3})+3(18^{2}+17^{2})-2\times 6^{2}(12+3)-5^{2}(10+3)\\
&-2\times 7^{2}(14+3)-4^{2}(8+3)]+\frac{1}{6}(7\times 18\times 17-7^{2}\times 28)=\frac{1629}{4}
\end{align*}
Use the approximate rejection rule. The value of $A_{3}^{*}$ is given by
$$A_{3}^{*}=\frac{A_{3}-E_{0}(A_{3})}{\sqrt{Var_{0}(A_{3})}}=\frac{142-201/2}{\sqrt{1629/4}}=2.056$$
When the sample size is large, $A_{3}^{*}\sim N(0,1)$ approximately. Since $Z_{\alpha}=z_{0.05}=1.645<A_{3}^{*}$, hence we reject the null hypothesis $H_{0}:\tau_{1}=\cdots=\tau_{5}$ at 5\% level of significance.

~\\
(c) Test $H_{0}$ against $H_{1}:\tau_{1}\leqslant\tau_{2}\leqslant\tau_{3}\geqslant\tau_{4}\geqslant\tau_{5}$ with at least one strict inequality by the Mack-Wolfe test, where the peak $p$ is unknown.

By definition, $U_{\cdot q}=\sum_{i\neq q}U_{iq}$. Since  $U_{vu}=n_{u}n_{v}-U_{uv}$ for $u<v$, it is easy to obtain $U_{21}=4$, $u_{31}=6$, $u_{41}=4$, $U_{51}=4$, $U_{32}=7$, $U_{42}=9$, $U_{52}=12$, $U_{43}=12$, $U_{53}=26$, $U_{54}=14$. Then we have
\begin{align*}
&U_{\cdot 1}=U_{21}+U_{31}+U_{41}+U_{51}=4+6+4+4=18\\
&U_{\cdot 2}=U_{12}+U_{32}+U_{42}+U_{52}=26+7+9+12=54\\
&U_{\cdot 3}=U_{13}+U_{23}+U_{43}+U_{53}=36+28+12+26=102\\
&U_{\cdot 4}=U_{14}+U_{24}+U_{34}+U_{54}=20+11+16+14=61\\
&U_{\cdot 5}=U_{15}+U_{25}+U_{35}+U_{45}=32+18+16+10=76\\
\end{align*}
The mean and variance of $U_{\cdot q}$ are given by $$E_{0}(U_{\cdot q})=\frac{n_{q}(N-n_{q})}{2} \text{, } Var_{0}(U_{\cdot q})=\frac{n_{q}(N-n_{q})(N+1)}{12}$$ for $q=1,2,3,4,5$. Then we have
\begin{align*}
&E_{0}(U_{\cdot 1})=\frac{6(28-6)}{2}=66 \text{, } Var_{0}(U_{\cdot 1})=\frac{6(28-6)29}{12}=319\\
&E_{0}(U_{\cdot 2})=\frac{5(28-5)}{2}=\frac{115}{2} \text{, } Var_{0}(U_{\cdot 2})=\frac{5(28-5)29}{12}=\frac{3335}{12}\\
&E_{0}(U_{\cdot 3})=\frac{7(28-7)}{2}=\frac{147}{2} \text{, } Var_{0}(U_{\cdot 3})=\frac{7(28-7)29}{12}=\frac{1421}{4}\\
&E_{0}(U_{\cdot 4})=\frac{4(28-4)}{2}=48 \text{, } Var_{0}(U_{\cdot 4})=\frac{4(28-4)29}{12}=232\\
&E_{0}(U_{\cdot 5})=\frac{6(28-6)}{2}=66 \text{, } Var_{0}(U_{\cdot 5})=\frac{6(28-6)29}{12}=319\\
\end{align*}
Since $U_{\cdot q}^{*}$ is defined as $$U_{\cdot q}^{*}=\frac{U_{\cdot q}-E_{0}(U_{\cdot q})}{\sqrt{Var_{0}(U_{\cdot q})}}$$
Then we can calculate $U_{\cdot 1}^{*},U_{\cdot 2}^{*},U_{\cdot 3}^{*},U_{\cdot 4}^{*},U_{\cdot 5}^{*}$ as below
\begin{align*}
&U_{\cdot 1}^{*}=\frac{18-66}{\sqrt{319}}=-2.687 \text{, } U_{\cdot 2}^{*}=\frac{54-115/2}{\sqrt{3335/12}}=-0.210\\
&U_{\cdot 3}^{*}=\frac{102-147/2}{\sqrt{1421/4}}=1.512 \text{, } U_{\cdot 4}^{*}=\frac{61-48}{\sqrt{232}}=0.853\\
&U_{\cdot 5}^{*}=\frac{76-66}{\sqrt{319}}=0.560\\
\end{align*}
The maximum value among $\{U_{\cdot 1}^{*},U_{\cdot 2}^{*},U_{\cdot 3}^{*},U_{\cdot 4}^{*},U_{\cdot 5}^{*}\}$ is $U_{\cdot 3}^{*}$, hence $\hat{p}=3$. Based on part (b), we have $A_{\hat{p}}=142$ and $A_{\hat{p}}^{*}=2.056$, where $\hat{p}=3$.\\
~\\
Use R program as below\\

\indent $>$ cUmbrPU(0.1,c(6,5,7,4,6))\\
~\\
\indent Monte Carlo Approximation (with 10000 Iterations) used:\\
~\\
\indent Group sizes: 6 5 7 4 6\\
\indent For the given alpha=0.1, the upper cutoff value is Mack-Wolfe Peak\\ 
\indent Unknown A*(p-hat)=1.9629727426, with true alpha level=0.0986\\
~\\
Since $Pr(A_{\hat{p}}^{*}\geqslant1.963)=0.099$, then $Pr(A_{\hat{p}}^{*}\geqslant2.056)<0.1$. Hence we reject the null hypothesis $H_{0}:\tau_{1}=\cdots=\tau_{5}$ at 10\% level of significance.

~\\
(d) Based on the results in parts (a)-(c), the null hypothesis $H_{0}$ is rejected in favor of the ordered alternatives at the 1\% level, and the null hypothesis $H_{0}$ is rejected in favor of the umbrella alternatives at the 5\% level. According to the level of significance, the ordered alternatives have the strongest support from the data.

The results of (a)-(c) are consistent. Since the null hypothesis $H_{0}$ is rejected in both cases, the only difference is the alternative hypothesis. In part (a), we accept the ordered alternatives. In parts (b) and (c), we accept the umbrella alternatives. Though we obtain the different conclusion, but they are both result from the fact that there is sufficient evidence to against $H_{0}$. In the case of alternatives, there is more evidence to against $H_{0}$ than the case of umbrella alternatives, which is not contradictory to each other.

~\\
\indent \textbf{Question 6}\\
~\\
(a) \textit{Proof.} Since $E(r_{ij})=\frac{s+1}{2}I_{\{c_{ij}=1\}}$ and $R_{j}=r_{1j}+\cdots+r_{nj}$, then
$$E(R_{j})=\sum_{i=1}^{n}E(r_{ij})=\sum_{i=1}^{n}\frac{s+1}{2}I_{\{c_{ij}=1\}}=\frac{p(s+1)}{2}$$
Since $r_{1j},\cdots,r_{nj}$ are in different blocks, they are independent. Based on $Var(r_{ij})=\frac{(s+1)(s-1)}{12}I_{\{c_{ij}=1\}}$, then
$$Var(R_{j})=\sum_{i=1}^{n}Var(r_{ij})=\sum_{i=1}^{n}\frac{(s+1)(s-1)}{12}I_{\{c_{ij}=1\}}=\frac{p(s+1)(s-1)}{12}$$
The Durbin-Skillings-Mack test statistic is given by
$$D=\frac{12}{\lambda k(s+1)}\sum_{j=1}^{k}\left(R_{j}-\frac{p(s+1)}{2}\right)^{2}$$
Then we have
\begin{align*}
E(D)&=\frac{12}{\lambda k(s+1)}\sum_{j=1}^{k}E\left[\left(R_{j}-\frac{p(s+1)}{2}\right)^{2}\right]\\
&=\frac{12}{\lambda k(s+1)}\sum_{j=1}^{k}E\left[\left(R_{j}-E(R_{j})\right)^{2}\right]\\
&=\frac{12}{\lambda k(s+1)}\sum_{j=1}^{k}Var(R_{j})\\
&=\frac{12}{\lambda k(s+1)}\cdot k\cdot\frac{p(s+1)(s-1)}{12}\\
&=\frac{p(s-1)}{\lambda}=k-1
\end{align*}
where $\lambda(k-1)=p(s-1)$ in BIBD. Hence $E(D)=k-1$.

~\\
(b) Based on data $\{X_{ij}\}$ in an incomplete block design, we can get ranks within each block $i$, for $i=1,2,3,4,5$.
\begin{align*}
&\text{Block 1: }21(3) \text{, } 15(1) \text{, } 17(2) \text{, } -(0) \text{, } 28(4)\\
&\text{Block 2: }25(2) \text{, } -(0) \text{, } 19(1) \text{, } 35(4) \text{, } 32(3)\\
&\text{Block 3: }39(3) \text{, } 32(1) \text{, } 35(2) \text{, } 44(4) \text{, } -(0)\\
&\text{Block 4: }-(0) \text{, } 22(2) \text{, } 16(1) \text{, } 24(3) \text{, } 30(4)\\
&\text{Block 5: }38(2) \text{, } 34(1) \text{, } -(0) \text{, } 45(4) \text{, } 42(3)
\end{align*}
Then we have $R_{1}=10$, $R_{2}=5$, $R_{3}=6$, $R_{4}=15$, $R_{5}=14$, and this BIBD has $k=5$, $n=5$, $s=4$, $p=4$, $\lambda=3$. By the definition of $D$,
\begin{align*}
D&=\frac{12}{\lambda k(s+1)}\sum_{j=1}^{k}R_{j}^{2}-\frac{3(s+1)p^{2}}{\lambda}\\
&=\frac{12}{3 (5)(4+1)}(10^{2}+5^{2}+6^{2}+15^{2}+14^{2})-\frac{3(4+1)4^{2}}{3}=13.12
\end{align*}
The test statistic $D\sim \chi_{4}^{2}$ approximately, and $\chi_{4,0.05}^{2}=11.668<D$, hence we reject the null hypothesis $H_{0}:\tau_{1}=\cdots=\tau_{5}$ in favor of the general alternatives at 5\% level of significance.

~\\
(c) The Skillings-Mack two-sided all-treatment multiple comparison procedure for BIBD is: for each pair $(\tau_{u},\tau_{v})$ with $u<v$,
\begin{align*}
&\text{Decide }\tau_{u}\neq\tau_{v}\text{ if } |R_{u}-R_{v}|\geqslant q_{\alpha}\sqrt{\frac{(s+1)(ps-s+\lambda)}{12}};\\
&\text{Otherwise accept}\tau_{u}=\tau_{v}.
\end{align*}
Given that $q_{0.1}=3.479$ for $k=5$, we can calculate
$$q_{0.1}\sqrt{\frac{(s+1)(ps-s+\lambda)}{12}}=3.479\sqrt{\frac{(4+1)(4\times 4-4+3)}{12}}=6.737$$
Based on values of $R_{j}$, $j=1,\cdots,5$, we have
\begin{align*}
&|R_{2}-R_{4}|=10 \text{, } |R_{2}-R_{5}|=9\\
&|R_{3}-R_{4}|=9 \text{, } |R_{3}-R_{5}|=8
\end{align*}
Since the values above are all larger than $6.737$, then we decide $\tau_{2}\neq\tau_{4}$, $\tau_{2}\neq\tau_{5}$, $\tau_{3}\neq\tau_{4}$, $\tau_{3}\neq\tau_{5}$ at $\alpha=0.1$.
\begin{align*}
&|R_{1}-R_{2}|=5 \text{, } |R_{1}-R_{3}|=4\\
&|R_{1}-R_{4}|=5 \text{, } |R_{1}-R_{5}|=4\\
&|R_{2}-R_{3}|=1 \text{, } |R_{4}-R_{5}|=1
\end{align*}
Since the values above are all smaller than $6.737$, then we accept $\tau_{1}=\tau_{2}$, $\tau_{1}=\tau_{3}$, $\tau_{1}=\tau_{4}$, $\tau_{1}=\tau_{5}$, $\tau_{2}=\tau_{3}$, $\tau_{4}=\tau_{5}$ at $\alpha=0.1$.

\end{document}